### 評価指標について

・評価指標は複数ではなく1つに絞ること
　（PrecisionとRecallを両方つかうとどちらかが犠牲になる。あちらがたてばこちらが立たず状態。両方考慮したければF1score。）
・とはいえ複数の評価指標を使いたいときもある
　‐Runnnig Time等
　→最適化基準　/ 満足基準　を設定する
　　Running Timeが100ms未満（満足基準）の中でAccuracy（最適化基準）を最大化

### Train/dev/test
・devセットとtestセットのデータの分布はできるだけ近いものにすること
・データの分け方は古典的には6:2:2だが…（で0タセットが100~10000くらい）
・現在は大量にデータを使える時代
　→100万サンプルあれば、98:1:1くらいでもよい（testセットで1万サンプルになる）
・システム開発が完了した後、テストセットが最終システムの品質を評価するに足りうるサイズがあれば十分。

### コストの修正
・実験を始めて、新たに評価軸が生まれた際は、柔軟にエラー指標を修正してよい
・例えば不適切な画像（ポルノ画像等）を分類してしまった場合だけ、ペナルティとしてエラーの値を10倍するようにコストを修正、等
・訓練データでは高解像度の画像を使用していたが、実運用の際には低解像度の画像が実際使われた。その場合、訓練時には少し劣るようなモデルが、実運用時には強くなる場合もある。

### ベイズエラー
・Bayes Optimal Error：理論的な限界点
・人間のパフォーマンスはこのベイズエラーに近いため、人間のパフォーマンスをベイズエラーライクに利用することが多い
・人間が得意とするタスクの場合、機械学習アルゴリズムが人間のパフォーマンスより劣る場合、ラベル付けされたデータを人間から取得することができる

### Error Analysis
・HumanError⇔Training ErrorとTraining Error ⇔ Dev Error
・比較してHuman⇔Trainingのほうが大きい　
　→Avoid Bias
　　‐より良い最適化手法を試す
　　‐アーキテクチャを複雑にする
　　‐より良いパラメータを探す
・比較してTrain/Devのほうが大きい
　→Avoid Variance
　　‐データを増やす
　　‐Regularization（Dropout, DataAugumentation...）
　　‐ネットワークを縮小する etc

### (機械が人間を上回るケース)
・自然な近くの問題ではないもの（ユーザーのクリックのデータ、AからBまでの所要時間のデータ、以前の融資申し込みのデータ…）
・人は自然な認知の問題が得意（目で見て、耳で聞いて…）
・膨大な量をさばかないといけないケースは機械得意 